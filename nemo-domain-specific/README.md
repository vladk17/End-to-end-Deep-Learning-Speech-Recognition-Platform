# Domain Specific – NeMo ASR Application (restored from the running docker image)
https://ngc.nvidia.com/catalog/containers/nvidia:nemo_asr_app_img

The Domain Specific - NeMo Automatic Speech Recognition (ASR) Application facilitates training, evaluation and performance comparison of ASR models. This NeMo application enables you to train or **fine-tune** pre-trained (acoustic and language) **ASR models** with your **own data**. Through this application, we empower you to create (on-prem) your own ASR models built on your domain specific audio data, that may be sensitive and thus not suitable for cloud ASR solutions. This gives you the ability to progressively create better performing ASR models specifically built for your data.

The Domain Specific - NeMo ASR application is a packaged and easy to use end-to-end ASR system that facilitates:
-   Acoustic Model training (and finetuning) on your own data.
-   Addition of a Language Model, as well as Language Model training on your own data.
-   Transcription (speech to text) or inference with each model created.
-   Multi-model comparison that facilitates the understanding of performance progression.

We use the [NVIDIA Neural Modules: NeMo](https://nvidia.github.io/NeMo/index.html) as the underlying ASR engine. NeMo is a framework-agnostic toolkit created for building AI applications for conversational AI. Through modular Deep Neural Networks (DNN) development, [NeMo](https://github.com/NVIDIA/NeMo) enables fast experimentation by connecting modules, mixing and matching components. NeMo modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations.

# Table of Contents
-   [Installation and Getting Started](#installation-and-getting-started)
-   [Automatic Speech Recognition Model and Pipeline](#automatic-speech-recognition-model-and-pipeline)
-   [Domain ASR Application](#domain-asr-application)

# Installation and Getting Started
Getting stated with the `nemo_asr_app` is very simple, as shown next.

## Download container from NGC
You can obtain the application by downloading the container from NGC. The image contains the complete Domain Specific NeMo ASR application (including notebooks, tools and scripts).
``` bash
docker pull nvcr.io/nvstaging/nemo/nemo_asr_app_img:v1.0
```
### Clone repository
Alternatively, you can clone the `nemo_asr_app` repository and follow the running instructions below.

### Download pre-trained models
To get a head start you download the pre-trained models used and created by this application. The Step 0 notebook explains how each of these models are used.

1. Pre-trained acoustic QuartzNet model used as a starting point for this application, can be downloaded [here](https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5).
2. The acoustic model created in Step 1, which finetunes the pre-trained acoustic model with WSJ data, can be downloaded [here](https://ngc.nvidia.com/catalog/models/nvidia:wsj_quartznet_15x5).
3. The language model trained on WSJ data in Step 2 can be downloaded [here](https://ngc.nvidia.com/catalog/models/nvidia:wsj_lm_decoder).

## Running the application
You can run the application using the [`./run_app.sh` script](run_app.sh), which will prompt you for a `Data directory` and `Port number`.
- The `Data directory` will hold all the models, datasets and outputs generated by the tool and thus requires large storage (ASR models can take multiple Gigabytes). The data directory can be at any location within your system, and it will be automatically mounted inside the container.
- The `port` number will be used to access the application notebooks: `[host system ip]:[port]`

The [run_app.sh](run_app.sh) script encapsulates a typical process of starting a container. It builds (if necessary) and runs a docker image based on a Dockerfile which is built on top of the [NGC PyTorch container](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch).

Note, to build the image yourself, use the docker build command inside the `./run_app.sh` script.

### Running from NGC NeMo ASR Application image
If you downloaded the container image through NGC you can run the application using the docker run command explained below, also available inside the `run_app.sh` script.

#### Environment variables
We must set the following environment variables to be able to use the user interface and to set the data directory for the application to save outputs and models.
```bash
# Variables to enable remote UI access
HOST_IP=`hostname -I | awk '{print $1;}'`
UI_PORT=8060
INPUT_PORT=8888 #port used to access notebooks
```
Mounting the data directory and saving it as an environment variable is necessary for the application to run. Note, the application gives you the freedom to set the data directory anywhere in your system.
```bash
DATA_DIR="raid/datasets/asr/data"
```
#### Entrypoint
We set the entrypoint to be JupyterLab, which provides access to the application notebooks using the host ip and port.
```bash
ENTRY="jupyter lab --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --NotebookApp.custom_display_url=http://$HOST_IP:$INPUT_PORT"
```
Alternatively, you can set the entry point to be the container’s terminal.
```bash
ENTRY="/bin/bash"
```
#### Docker run
Here you can see the complete run command.

```bash
# APP settings
DATA_DIR="raid/datasets/asr/data"
IMAGE_NAME='nemo_asr_app_img'
USERNAME=`whoami`
CONTAINER_NAME='run_nemo_asr_app_cont_'$USERNAME
ENTRY="jupyter lab --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --NotebookApp.custom_display_url=http://$HOST_IP:$INPUT_PORT"

# docker command
DOCKER_CMD=docker
if hash nvidia-docker 2>/dev/null; then
  DOCKER_CMD=nvidia-docker
fi

# Variables to enable remote UI access
HOST_IP=`hostname -I | awk '{print $1;}'`
UI_PORT=8060
INPUT_PORT=8888 #port used to access notebooks

# Run command
$DOCKER_CMD run -it --rm --name $CONTAINER_NAME \
      --ipc=host \
        --env UI_HOST_IP=$HOST_IP \
        --env UI_PORT=$UI_PORT \
        --env C_PORT=$INPUT_PORT \
        --env DATA_DIR=$DATA_DIR \
        -v $DATA_DIR:$DATA_DIR \
      -p $INPUT_PORT:8888 \
        -p $UI_PORT:$UI_PORT \
      $IMAGE_NAME $ENTRY
```

## User Interface
To run the *User Interface* you must be *inside the container* and simply run the `user_interface/app.py` script from the container's terminal.
```bash
python /tmp/nemo_asr_app/user_interface/app.py
```

## Code Structure
This repository is divided into the following sub-folders each serving different functionalities.

* [notebooks](notebooks): Application’s notebooks that walk through the process of ASR model training and evaluation.
* [tools](tools): Scripts and tools that enable the system's functionality.
* [user_interface](user_interface): User Interface files.
* [run_app.sh](run_app.sh) and [Dockerfile](Dockerfile): Scripts to run the application.

# Automatic Speech Recognition Model and Pipeline
The Automatic Speech Recognition (ASR) NeMo model used in this application is based on a [Jasper]( https://arxiv.org/abs/1904.03288)-like model named [QuartzNet]( https://nvidia.github.io/NeMo/asr/quartznet.html).  The QuartzNet model can achieve Jasper’s performance but with a lot less parameters (form about 333M to about 19M). This model consists of separable convolutions and larger filters, often denoted by QuartzNet_[BxR], where B is the number of blocks, and R - the number of convolutional sub-blocks within a block. Each sub-block contains a 1-D separable convolution, batch normalization, ReLU, and dropout. To learn more about NeMo’s ASR models see [tutorial](https://nvidia.github.io/NeMo/asr/models.html) and [paper](https://arxiv.org/pdf/1910.10261.pdf).

Jasper and QuartzNet are a [CTC-based]( https://www.cs.toronto.edu/~graves/icml_2006.pdf) end-to-end model, which can predict a transcript directly from an audio input, without additional alignment information.

If you wish to learn how the application builds the NeMo ASR engine, specifically the training and evaluation workflows, refer to the [jasper_train.py](/tools/NeMo/jasper_train.py) and [jasper_eval.py](/tools/NeMo/jasper_eval.py) scripts inside the tools folder.

### ASR-CTC pipeline
![CTC pipeline](https://miro.medium.com/max/4131/1*4ailcffDrQH3v9FquuX30A.png)
Image Source: [CTC Networks and Language Models: Prefix Beam Search Explained](https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306)

The typical ASR-CTC pipeline is shown in the Figure above. Here, the CTC network creates a probability CTC matrix, from the audio input, where columns represent a timestep and rows correspond to a letter in our alphabet, note the probabilities of each column (across all letters) sum to 1. For prediction using max decoding or greedy decoding, the letter with the highest probability at each timestep is chosen, in other words a temporal softmax output layer is used. Next, the repeated characters are removed or collapsed, and blank tokens are discarded. Additionally, a language model can be used to solve ambiguities in the in the transcription or softmax output, with the help of linguistic knowledge provided by prefix beam search. To learn more see [link]( https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306).

The CTC-ASR training pipeline followed by NeMo is shown in the following figure:
![NeMo CTC-ASR](https://nvidia.github.io/NeMo/_images/ctc_asr.png)
Image Source: [NVIDIA Neural Modules: NeMo](https://nvidia.github.io/NeMo/)

This includes:
-   audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)
-   neural acoustic model (which predicts a probability distribution P_t(c) over vocabulary characters c per each time step t given input features per each timestep)
-   CTC loss function

# Domain ASR Application
In our application, we enable a complete end-to-end workflow to enable domain adaptation of ASR models using your own data. This is done through 3 easy steps:
1.  The effect of `Acoustic Model` training
2.  The effect of adding a `Language Model`
3.  Model performance comparison with Word Error Rate (WER)

We exemplify the end-to-end process of ASR domain adaptation through different example use-cases. We use a model pre-trained on open-source `English` datasets [LibriSpeech](http://www.openslr.org/12) and [English - Mozilla Common Voice](https://voice.mozilla.org/en/datasets) found in the [NVIDIA GPU Cloud]( https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5) and fine-tune the pre-trained models and evaluate these with domain specific datasets.

## Project Tracking of Data, Models and Configurations
To simplify and enable reproducibility of the ASR workflow, our application allows you to we create a `project` which enables the tracking of datasets, models and configurations across all workflows – i.e. acoustic model and language model workflows.
Everything related to a project is saved in disc in a manifest that can be access through its `project_id`.
At the start of the project, the manifest is pre-populated with the baseline pre-trained models. For acoustic model we use a pretrained model on LibriSpeech with a greedy decoder and for language model we use a Baidu's CTC decoder model. Both pre-trained models are built or downloaded in Step 0 and thus are ready and available for Steps 1-3.

## Step 0. Models and Data - Download and Preparation
Before getting started with the 3-step ASR workflow you need to download & prepare the data and pre-trained models. We have included the necessary tools and an example notebook to guide you through the preparation process for our example use-case.
The steps followed in this [notebook](/notebooks/WSJ/Step%200.%20Models%20and%20Data%20-%20Download%20and%20Preparation.ipynb) are:
1. Create a folder named `example_data` inside the `data directory` to host pre-trained models and example datasets.
2. Download the `pre-trained` acoustic model from Nvidia GPU Cloud (NGC).
3. Build baseline Language Model:  [Baidu's CTC decoded with LM implementation](https://github.com/PaddlePaddle/DeepSpeech), specifically a 6-gram KenLM model trained on LibriSpeech.
4. Download the WSJ trained models created by this application from NGC or alternatively the dataset for acoustic model and language model training (fine-tuning) and understand how to build NeMo ready datasets.

## Step 1. The effect of Acoustic Model training
In the first step of our workflow, we evaluate the effect of `Acoustic Model training`. In this [notebook](/notebooks/WSJ/Step%201.%20The%20effect%20of%20Acoustic%20Model%20training.ipynb) we walk you through the process of comparing a baseline pre-trained acoustic model with an acoustic model finetuned on domain specific data.
1. We first train an acoustic model on a domain specific dataset. More specifically, we fine-tune a pre-trained baseline model, where we perform knowledge transfer from a [model trained on LibriSpeech with EN Common Voice](https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5) and train it with a domain specific dataset.
2. We compare the performance of the baseline model and the fine-tuned model by performing inference with both models and compare their WER performance.

## Step 2. The effect of adding a Language Model
Up to now, the output or transcript the ASR system is generated by an “end-to-end” [CTC-based network]( https://www.cs.toronto.edu/~graves/icml_2006.pdf) which matches audio and text without additional alignment information. However, ambiguities in the transcription, for example when collapsing repeated characters and removing blanks, can exist as the CTC-based network has little prior linguistic knowledge. This where a language model comes in, as it can help solve those decoding ambiguities.
Specifically, the decoder output or transcript, when introducing a language model, is dependent on both the CTC network (softmax) output and the language model. The language model we use is based on prefix beam search [KenLM](https://github.com/kpu/kenlm) which imposes a language model constraint on the new predicted character based on previous (most probable) prefixes. To learn more see [First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs](https://arxiv.org/pdf/1408.2873.pdf).
In this [notebook](/notebooks/WSJ/Step%202.%20The%20effect%20of%20adding%20a%20Language%20Model.ipynb) we evaluate the effect in performance of `adding a Language Model `. The model we use and train is the [Baidu’s CTC decoder with N-Gram LM implementation](https://github.com/PaddlePaddle/DeepSpeech).

## Step 3 - Performance Comparison of Models’ `Word Error Rate`
Now that you have trained and evaluated multiple ASR models on your own data, we can compare them in one place. For this, we provide a [notebook](/notebooks/WSJ/Step%203.%20Performance%20Comparison%20with%20WER.ipynb) that compares the WER performance of the pre-trained models and the fine-tuned models across both acoustic and language model workflows.
