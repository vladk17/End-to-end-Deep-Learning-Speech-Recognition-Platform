{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloads and Preparations for Example Use Case\n",
    "\n",
    "In this notebook we download and prepare the models and data required for the example use-case. \n",
    "\n",
    "To exemplify the end-to-end Domain Specific NeMo ASR application, we start with an acoustic model pre-trained on open-source English datasets [LibriSpeech](http://www.openslr.org/12) and [English - Mozilla Common Voice](https://voice.mozilla.org/en/datasets). Then we fine-tune the pre-trained acoustic and language models with **Wall Street Journal (WSJ)** news dataset. Through this example case, we show we can easily do transfer learning or domain adaptation from [relatively old] fiction books (LibriSpeech) to [relatively modern] business news (WSJ).\n",
    "\n",
    "The steps followed in this notebook are:\n",
    "1. Create a folder named `example_data` inside the `data directory` you input when starting the container.\n",
    "2. Get the `pre-trained` acoustic model from Nvidia GPU Cloud (NGC).\n",
    "3. Build a baseline Language Model (6-gram KenLM model trained on LibriSpeech)\n",
    "4. Download the WSJ trained models created by this application from NGC or alternatively the dataset for *acoustic model* and *language model* training (fine-tuning) and understand how to build NeMo ready datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ['APP_DIR']='..'\n",
    "os.environ['DATA_DIR']=os.path.join(os.environ['APP_DIR'],'data')\n",
    "sys.path.append(os.environ['APP_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# required imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from tools.System.config import cfg\n",
    "from tools.filetools import mkdir_p\n",
    "from tools.misc import create_lm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create `example_data` folder\n",
    "We use the `example_data` folder within the data directory to host all pre-trained models and example datasets used in our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected path for 'example_data' folder: ../data/example_data\n",
      "Created example_data folder at:  ../data/example_data\n"
     ]
    }
   ],
   "source": [
    "# expected data path\n",
    "print(\"Expected path for 'example_data' folder:\", cfg.DATASET.PATHS.EXAMPLE_DATA)\n",
    "\n",
    "# create folder\n",
    "mkdir_p(cfg.DATASET.PATHS.EXAMPLE_DATA)\n",
    "print(\"Created example_data folder at: \", cfg.DATASET.PATHS.EXAMPLE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download pre-trained model from NGC\n",
    "\n",
    "In this ASR workflow we use the QuartzNet model, a high-performing yet small end-to-end neural acoustic model for automatic speech recognition. Learn more about QuartzNet here: [tutorial](https://nvidia.github.io/NeMo/asr/quartznet.html) and [paper](https://arxiv.org/pdf/1910.10261.pdf). \n",
    "\n",
    "### QuartzNet 15x5 for NeMo\n",
    "You can find this pre-trained model inside the demo folder: `/tmp/nemo_asr_app/demo/pre-trained/`\n",
    "\n",
    "You can also download it from:\n",
    "https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5\n",
    "\n",
    "QuarzNet is a Jasper-like network which uses separable convolutions and larger filter sizes. It has comparable accuracy to Jasper while having much fewer parameters. This particular model has 15 blocks each repeated 5 times.\n",
    "\n",
    "QuartzNet15x5 Encoder and Decoder neural module's checkpoints are trained using Neural Modules(NeMo) toolkit. NVIDIAâ€™s Apex/Amp O1 optimization level was used for training on 8xV100 GPUs. These modules were trained using LibriSpeech (+-10% speed perturbation) and Mozilla's EN Common Voice \"validated\" set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Build Baseline Language Model\n",
    "Next, we provide the command and required scripts (`build_6-gram_OpenSLR_lm.sh`) to build a baseline language model for any ASR application. This language model uses [Baidu's CTC decoder with LM implementation](https://github.com/PaddlePaddle/DeepSpeech), specifically a **6-gram KenLM model** trained on **LibriSpeech**.\n",
    "\n",
    "**Note**: If you wish to use a language model in your ASR pipeline, you need to install the necessary software. Please refer to the Dockerfile in `/tmp/nemo_asr_app/Dockerfile` and re-build the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! cd ../tools/NeMo && ./build_6-gram_OpenSLR_lm.sh ../data/example_data\n"
     ]
    }
   ],
   "source": [
    "cmd = \"! cd \"+ cfg.NEMO.TOOLS + \" && ./build_6-gram_OpenSLR_lm.sh \" + cfg.DATASET.PATHS.EXAMPLE_DATA\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'BASH_FUNC_module%%': '() {  _moduleraw \"$*\" 2>&1\\n}',\n",
       "        'LESSOPEN': '| /usr/bin/lesspipe %s',\n",
       "        'SSH_CLIENT': '37.142.5.127 11143 22',\n",
       "        'LOGNAME': 'ubuntu',\n",
       "        'USER': 'ubuntu',\n",
       "        'PATH': '/home/ubuntu/.local/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/src/cntk/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin',\n",
       "        'HOME': '/home/ubuntu',\n",
       "        'LD_LIBRARY_PATH': '/home/ubuntu/src/cntk/bindings/python/cntk/libs:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/opt/amazon/efa/lib:/usr/local/mpi/lib:',\n",
       "        'SSH_CONNECTION': '37.142.5.127 11143 172.31.45.100 22',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'TERM': 'xterm-color',\n",
       "        'SHELL': '/bin/bash',\n",
       "        'MODULEPATH_modshare': '/usr/share/modules/$MODULE_VERSION/modulefiles:1:/etc/environment-modules/modules:1:/usr/share/modules/modulefiles:1:/usr/share/modules/versions:1',\n",
       "        'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop',\n",
       "        'MANPATH': '/opt/aws/neuron/share/man:',\n",
       "        'MODULESHOME': '/usr/share/modules',\n",
       "        'XDG_RUNTIME_DIR': '/run/user/1000',\n",
       "        'JPY_PARENT_PID': '23340',\n",
       "        'PYTHONPATH': '/home/ubuntu/src/cntk/bindings/python',\n",
       "        'BASH_FUNC__moduleraw%%': '() {  unset _mlre _mlIFS _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre:-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre:-}${_mlv}=\\'`eval \\'echo ${\\'$_mlrv\\':-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre:-}\" ]; then\\n _mlre=\"eval ${_mlre}\";\\n fi;\\n eval `${_mlre:-}/usr/bin/tclsh /usr/lib/x86_64-linux-gnu/modulecmd.tcl bash $*`;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n if [ -n \"${_mlshdbg:-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS _mlshdbg;\\n return $_mlstatus\\n}',\n",
       "        'ENV': '/usr/share/modules/init/profile.sh',\n",
       "        'PKG_CONFIG_PATH': '/usr/local/lib/pkgconfig:',\n",
       "        'BASH_ENV': '/usr/share/modules/init/bash',\n",
       "        'XDG_SESSION_ID': '16',\n",
       "        '_': '/usr/local/bin/jupyter',\n",
       "        'MODULEPATH': '/etc/environment-modules/modules:/usr/share/modules/versions:/usr/share/modules/$MODULE_VERSION/modulefiles:/usr/share/modules/modulefiles',\n",
       "        'BASH_FUNC_switchml%%': '() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/lib/x86_64-linux-gnu/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/lib/x86_64-linux-gnu/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}',\n",
       "        'LESSCLOSE': '/usr/bin/lesspipe %s %s',\n",
       "        'SSH_TTY': '/dev/pts/0',\n",
       "        'LOADEDMODULES': '',\n",
       "        'SHLVL': '1',\n",
       "        'PWD': '/home/ubuntu',\n",
       "        'MAIL': '/var/mail/ubuntu',\n",
       "        'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:',\n",
       "        'MODULES_CMD': '/usr/lib/x86_64-linux-gnu/modulecmd.tcl',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       "        'APP_DIR': '..',\n",
       "        'DATA_DIR': '../data'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ./build_6-gram_OpenSLR_lm.sh: Permission denied\n"
     ]
    }
   ],
   "source": [
    "! cd ../tools/NeMo && ./build_6-gram_OpenSLR_lm.sh ../data/example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy (select cmd + shift RMB + copy) below the command generated to build the KenLM model. Note, this can take some time, we recommend you run it inside the container's terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LM inside /raid/datasets/asr/data/example_data\n",
      "--2019-11-06 00:02:33--  http://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1507274412 (1.4G) [application/x-gzip]\n",
      "Saving to: â€˜librispeech-lm-norm.txt.gzâ€™\n",
      "\n",
      "librispeech-lm-norm 100%[===================>]   1.40G  5.16MB/s    in 4m 1s   \n",
      "\n",
      "2019-11-06 00:06:35 (5.97 MB/s) - â€˜librispeech-lm-norm.txt.gzâ€™ saved [1507274412/1507274412]\n",
      "\n",
      "*********************************************************\n",
      "Unigram tokens 803288729 types 973676\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:11684112 2:13311027200 3:24958175232 4:39933083648 5:58235744256 6:79866167296\n",
      "Statistics:\n",
      "1 973676 D1=0.647192 D2=1.04159 D3+=1.3919\n",
      "2 41161096 D1=0.723617 D2=1.06317 D3+=1.36127\n",
      "3 207278547 D1=0.804357 D2=1.09256 D3+=1.31993\n",
      "4 438095063 D1=0.876863 D2=1.15052 D3+=1.32047\n",
      "5 587120377 D1=0.93329 D2=1.23036 D3+=1.36071\n",
      "6 634027586 D1=0.95229 D2=1.42069 D3+=1.45621\n",
      "Memory estimate for binary LM:\n",
      "type       MB\n",
      "probing 40059 assuming -p 1.5\n",
      "probing 47351 assuming -r models -p 1.5\n",
      "trie    20999 without quantization\n",
      "trie    12124 assuming -q 8 -b 8 quantization \n",
      "trie    17737 assuming -a 22 array pointer compression\n",
      "trie     8863 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:11684112 2:658577536 3:4145570940 4:10514281512 5:16439370556 6:20288882752\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:11684112 2:658577536 3:4145570940 4:10514281512 5:16439370556 6:20288882752\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211450792 kB\tVmRSS:33688 kB\tRSSMax:54462840 kB\tuser:2268.42\tsys:574.454\tCPU:2842.87\treal:2450.72\n",
      "decoders/kenlm/build/bin/build_binary trie -q 8 -b 7 -a 256 /raid/datasets/asr/data/example_data/language_model/6-gram.arpa /raid/datasets/asr/data/example_data/language_model/6-gram-lm.binary\n",
      "Reading /raid/datasets/asr/data/example_data/language_model/6-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Identifying n-grams omitted by SRI\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Quantizing\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Writing trie\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# ! cd /home/adrianaf/projects/asr_system/nemo_asr_app/tools/NeMo && ./build_6-gram_OpenSLR_lm.sh /raid/datasets/asr/data/example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. WSJ dataset\n",
    "\n",
    "In this example use-case, we fine-tune the pre-trained acoustic and language models with Wall Street Journal news datasets. \n",
    "Through this use case, we show we can easily do transfer learning or domain adaptation from old fiction books (LibriSpeech) to business news (WSJ).\n",
    "\n",
    "This dataset is part of the Linguistic Data Consortium and can be found here:\n",
    "- CSR-I (WSJ0) Complete: https://catalog.ldc.upenn.edu/LDC93S6A\n",
    "- CSR-II (WSJ1) Complete: https://catalog.ldc.upenn.edu/LDC94S13A\n",
    "\n",
    "To use this dataset you must normalize the text, i.e. lowercase text, remove punctuations and change digits to text representation. We provide utility functions in `tools/transcript_tools.py` that can help you with dataset preparation.\n",
    "\n",
    "Note: To  download this dataset a license is required please refer to [LDC to learn more](https://www.ldc.upenn.edu/language-resources/data/obtaining). \n",
    "\n",
    "\n",
    "## WSJ Fine-tuned Model Checkpoints\n",
    "\n",
    "To help you walk through this example use-case, we provide you with the WSJ fine-tuned models for both acoustic and language models. These models can be found in NGC:\n",
    "1. The acoustic model created in Step 1, which finetunes the pre-trained acoustic model with WSJ data, can be downloaded from this [link](https://ngc.nvidia.com/models/nvidia:wsj_quartznet_15x5).\n",
    "2. The language model trained on WSJ in Step 2 can be downloaded from this [link](https://ngc.nvidia.com/models/nvidia:wsj_lm_decoder).\n",
    "\n",
    "When downloading the models, you can save these inside the folders listed below (which are automatically generated when running the Step 1 and Step 2 notebooks): \n",
    "- Acoustic model\n",
    "    - `[data_dir]/models/acoustic_models/WSJ/WSJ_finetuning-lr_0.0001-bs_16-e_100-wd_0.0-opt_novograd/checkpoints/`\n",
    "    - You can also find this finetuned model inside the demo folder: `/tmp/nemo_asr_app/demo/finetuned/`\n",
    "- Language model\n",
    "    - `[data_dir]/models/language_models/`[WS_lm.binary]\n",
    "    \n",
    "Alternatively, if you downloaded the WSJ data or for your own dataset you can follow the instructions below to create NeMo ready datasets.\n",
    "As well, these models can be use directly on your own data to perform inference.\n",
    "\n",
    "## 4.1 Create NeMo ready - Acoustic Model Dataset\n",
    "\n",
    "NeMo requires datasets to be in the format of:\n",
    "- `wav` audio clips with sampling rate (16000) and max clip duration (16.5) specified by the [configuration file](/tools/NeMo/example_configs/quartznet15x5.yaml).\n",
    "- The dataset format as a `json` file where each entry has the keys: `audio_filepath`, `duration` and `text`.\n",
    "\n",
    "You can see the script used to create NeMo datasets from common_voice datasets in `tools/NeMo/create_common_voice_dataset.py`. This script can help you correctly format your audio clips and json training dataset.\n",
    "\n",
    "## 4.2 Create NeMo ready - Language Model Dataset\n",
    "\n",
    "A Language Model (LM) can improve decoder's performance by resolving ambiguities in speech to text transcription, more information on how LM help ASR systems is provided in Step 2.\n",
    "\n",
    "The Language Model dataset has the pre-processed text from the WSJ dataset in a single column, this file is saved as a `.txt` file. You can see the example LibriSpeech LM dataset `example_data/language_model/librispeech-lm-norm.txt` which was used to create the baseline language model. To create the WSJ language model dataset you can use the pre-processed text from the WSJ training dataset and save it as a single column in  a `.txt` file.\n",
    "\n",
    "For this WSJ end-to-end use case we use 3 datasets:\n",
    "1. `wsj-train-si284-speed-0.9-1.1.json` for acoustic model training (Note: Audio speed perturbation helps build better models)\n",
    "2. `wsj-eval-92.json` or `wsj-dev-93.json` for model evaluation\n",
    "3. `wsj-lm-data.txt` for language model training\n",
    "\n",
    "You can use the commands below to check and correct the paths to the audio files inside the acoustic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n1 /raid/datasets/asr/data/example_data/wsj/wsj-dev-93.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now replace this path with the correct path inside the container:\n",
    "#!sed -i 's,/data,/raid/datasets/asr/data/example_data,g' /raid/datasets/asr/data/example_data/wsj/wsj-dev-93.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm change\n",
    "#!head -n1 /raid/datasets/asr/data/example_data/wsj/wsj-dev-93.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have downloaded and pre-process the necessary models and datasets to walk through our example use-case.\n",
    "\n",
    "Your `example_data` folder now contains 2 sub-folders: 1) `wsj` with finetuning data and 2) `language_model` with baseline LM model and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
